{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615abf57-ed5e-446d-bee9-9b1eaa7e4cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import optuna\n",
    "\n",
    "MAIN_PATH = \"/dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Global settings and seeding\n",
    "# ---------------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Global paths and constants\n",
    "IMPORTS_PATH = os.path.join(MAIN_PATH, \"data/import-volume.csv\")\n",
    "ADJACENCY_MATRIX = pd.read_parquet(os.path.join(MAIN_PATH, \"data/adjacency-matrix.parquet\"))\n",
    "BOOKINGS_PATH = os.path.join(MAIN_PATH, \"data/bookings_data.pkl\")\n",
    "\n",
    "# Global hyperparameters and settings\n",
    "use_validation = False  \n",
    "NODE_FEATURES = 19        # 6 from volume/week encoding + 13 booking features\n",
    "TIME_WINDOW_SIZE = 13\n",
    "LOADERS_WOKRES = 4\n",
    "\n",
    "MIN_HORIZON = 1\n",
    "MAX_HORIZON = 14\n",
    "\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "EARLY_STOP_DELTA = 0.001\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Data and Graph Preparation\n",
    "# ---------------------------\n",
    "def get_import_data():\n",
    "    \"\"\"Load and pivot the import volume data.\"\"\"\n",
    "    try:\n",
    "        import_data = pd.read_csv(IMPORTS_PATH)\n",
    "        import_data[\"week\"] = pd.to_datetime(import_data[\"week\"])\n",
    "        import_data[\"pool\"] = import_data[\"pool\"].astype(\"str\")\n",
    "        import_data[\"volume\"] = import_data[\"import\"].astype(\"float\")\n",
    "        import_data.drop(columns=[\"import\"], inplace=True)\n",
    "        import_data = import_data.sort_values(\"week\")\n",
    "        import_data = import_data.loc[\n",
    "            (import_data.week >= pd.to_datetime(\"2017-05-01\")) &\n",
    "            (import_data.week <= pd.to_datetime(\"2024-10-20\"))\n",
    "        ]\n",
    "        logging.info(f\"Imports data range from {import_data['week'].min().strftime('%Y-%m-%d')} \"\n",
    "                     f\"to {import_data['week'].max().strftime('%Y-%m-%d')}\")\n",
    "        import_data = import_data.pivot(index='week', columns='pool', values='volume').T\n",
    "        import_data = import_data.fillna(0)\n",
    "        return import_data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_import_data(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def get_graph_structure(threshold, a):\n",
    "    \"\"\"Construct the graph structure (edge_index and edge_weights) from the adjacency matrix.\"\"\"\n",
    "    try:\n",
    "        a = a.reset_index(drop=True)\n",
    "        a.columns = range(a.shape[1])\n",
    "        a = a.where(pd.notnull(a), a.T)\n",
    "        a = a.to_numpy()\n",
    "        a_filtered = (a < threshold).astype(np.int32)\n",
    "        edge_index = torch.nonzero(torch.tensor(a_filtered, dtype=torch.long), as_tuple=False).t()\n",
    "        edge_weights = []\n",
    "        for e in edge_index.numpy().T:\n",
    "            distance = a[e[0], e[1]]\n",
    "            edge_weights.append(distance)\n",
    "        edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_graph_structure(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def prepare_data(use_validation, prediction_horizon, batch_size):\n",
    "    \"\"\"Preprocess the data, create features, and return DataLoaders.\"\"\"\n",
    "    try:\n",
    "        data = get_import_data()\n",
    "        bookings = pickle.load(open(BOOKINGS_PATH, \"rb\"))\n",
    "        logging.info(f\"Bookings data range from {min(bookings.keys())} to {max(bookings.keys())}\")\n",
    "\n",
    "        if use_validation:\n",
    "            val_weeks = 52\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - val_weeks - test_weeks\n",
    "        else:\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - test_weeks\n",
    "\n",
    "        volume_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "        week_numbers = data.columns.to_series().dt.isocalendar().week.astype(np.float32).values\n",
    "\n",
    "        if use_validation:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            val_volume = volume_tensor[:, train_weeks: train_weeks + val_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks]\n",
    "        else:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        # Normalize volume using training statistics\n",
    "        train_mean = train_volume.mean(dim=1, keepdim=True)\n",
    "        train_std = train_volume.std(dim=1, keepdim=True) + 1e-6\n",
    "        train_volume_norm = (train_volume - train_mean) / train_std\n",
    "        if use_validation:\n",
    "            val_volume_norm = (val_volume - train_mean) / train_std\n",
    "        test_volume_norm = (test_volume - train_mean) / train_std\n",
    "\n",
    "        # Create week encoding features\n",
    "        week_numbers_tensor = torch.tensor(week_numbers, dtype=torch.float32)\n",
    "        sin_week = torch.sin(2 * np.pi * week_numbers_tensor / 52)\n",
    "        cos_week = torch.cos(2 * np.pi * week_numbers_tensor / 52)\n",
    "        holiday_ohe = (week_numbers_tensor == 52).long()\n",
    "        weeks_to_holiday = (52 - week_numbers_tensor) % 52\n",
    "        weeks_from_holiday = (week_numbers_tensor - 52) % 52\n",
    "\n",
    "        def split_features(feature, n_train, n_val, n_test):\n",
    "            train_feat = feature[:n_train]\n",
    "            val_feat = feature[n_train: n_train + n_val]\n",
    "            test_feat = feature[n_train + n_val: n_train + n_val + n_test]\n",
    "            return train_feat, val_feat, test_feat\n",
    "\n",
    "        if use_validation:\n",
    "            train_sin, val_sin, test_sin = split_features(sin_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_cos, val_cos, test_cos = split_features(cos_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_holiday, val_holiday, test_holiday = split_features(holiday_ohe, train_weeks, val_weeks, test_weeks)\n",
    "            train_to, val_to, test_to = split_features(weeks_to_holiday, train_weeks, val_weeks, test_weeks)\n",
    "            train_from, val_from, test_from = split_features(weeks_from_holiday, train_weeks, val_weeks, test_weeks)\n",
    "        else:\n",
    "            train_sin = sin_week[:train_weeks]\n",
    "            test_sin = sin_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_cos = cos_week[:train_weeks]\n",
    "            test_cos = cos_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_holiday = holiday_ohe[:train_weeks]\n",
    "            test_holiday = holiday_ohe[train_weeks: train_weeks + test_weeks]\n",
    "            train_to = weeks_to_holiday[:train_weeks]\n",
    "            test_to = weeks_to_holiday[train_weeks: train_weeks + test_weeks]\n",
    "            train_from = weeks_from_holiday[:train_weeks]\n",
    "            test_from = weeks_from_holiday[train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        def create_week_feature_tensor(week_feat, num_nodes):\n",
    "            if week_feat.dim() == 1:\n",
    "                week_feat = week_feat.unsqueeze(1)\n",
    "            return week_feat.unsqueeze(0).repeat(num_nodes, 1, 1)\n",
    "\n",
    "        num_nodes = 16  # Hard-coded based on the data\n",
    "        train_sin_feat = create_week_feature_tensor(train_sin, num_nodes)\n",
    "        train_cos_feat = create_week_feature_tensor(train_cos, num_nodes)\n",
    "        train_holiday_feat = create_week_feature_tensor(train_holiday, num_nodes)\n",
    "        train_to_feat = create_week_feature_tensor(train_to, num_nodes)\n",
    "        train_from_feat = create_week_feature_tensor(train_from, num_nodes)\n",
    "\n",
    "        if use_validation:\n",
    "            val_sin_feat = create_week_feature_tensor(val_sin, num_nodes)\n",
    "            val_cos_feat = create_week_feature_tensor(val_cos, num_nodes)\n",
    "            val_holiday_feat = create_week_feature_tensor(val_holiday, num_nodes)\n",
    "            val_to_feat = create_week_feature_tensor(val_to, num_nodes)\n",
    "            val_from_feat = create_week_feature_tensor(val_from, num_nodes)\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "        else:\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "\n",
    "        train_volume_feat = train_volume_norm.unsqueeze(2)\n",
    "        if use_validation:\n",
    "            val_volume_feat = val_volume_norm.unsqueeze(2)\n",
    "        test_volume_feat = test_volume_norm.unsqueeze(2)\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            val_data_combined = torch.cat([\n",
    "                val_volume_feat, val_sin_feat, val_cos_feat,\n",
    "                val_holiday_feat, val_to_feat, val_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "\n",
    "        # Process booking features\n",
    "        booking_list = []\n",
    "        for date in data.columns:\n",
    "            booking_df = bookings[date.strftime(\"%Y-%m-%d\")]\n",
    "            booking_list.append(booking_df.values)\n",
    "        booking_array = np.stack(booking_list, axis=1)\n",
    "        bookings_tensor = torch.tensor(booking_array, dtype=torch.float32)\n",
    "\n",
    "        if use_validation:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            val_bookings = bookings_tensor[:, train_weeks: train_weeks + val_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks, :]\n",
    "        else:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks: train_weeks + test_weeks, :]\n",
    "\n",
    "        booking_mean = train_bookings.mean(dim=(0, 1), keepdim=True)\n",
    "        booking_std = train_bookings.std(dim=(0, 1), keepdim=True) + 1e-6\n",
    "        train_bookings = (train_bookings - booking_mean) / booking_std\n",
    "        if use_validation:\n",
    "            val_bookings = (val_bookings - booking_mean) / booking_std\n",
    "        test_bookings = (test_bookings - booking_mean) / booking_std\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            val_data_combined = torch.cat([val_data_combined, val_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "\n",
    "        class TimeSeriesDataset(Dataset):\n",
    "            def __init__(self, data, window_size, horizon):\n",
    "                self.data = data\n",
    "                self.window_size = window_size\n",
    "                self.horizon = horizon  # Uses the provided prediction horizon\n",
    "                self.num_samples = data.shape[1] - window_size - (horizon - 1)\n",
    "\n",
    "            def __len__(self):\n",
    "                return self.num_samples\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                x = self.data[:, idx: idx + self.window_size, :].transpose(0, 1)\n",
    "                y = self.data[:, idx + self.window_size + self.horizon - 1, 0]\n",
    "                return x, y\n",
    "\n",
    "        window_size = TIME_WINDOW_SIZE\n",
    "        train_dataset = TimeSeriesDataset(train_data_combined, window_size, horizon=prediction_horizon)\n",
    "        test_dataset = TimeSeriesDataset(test_data_combined, window_size, horizon=prediction_horizon)\n",
    "        if use_validation:\n",
    "            val_dataset = TimeSeriesDataset(val_data_combined, window_size, horizon=prediction_horizon)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                  pin_memory=True, num_workers=LOADERS_WOKRES)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "        if use_validation:\n",
    "            val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, \n",
    "                                    pin_memory=True)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_data(): \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "def prepare_graph(threshold):\n",
    "    \"\"\"Prepare the graph structure (edge_index, edge_weights) using the given threshold.\"\"\"\n",
    "    try:\n",
    "        edge_index, edge_weights = get_graph_structure(threshold, ADJACENCY_MATRIX)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_graph(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definitions\n",
    "# ---------------------------\n",
    "class GNNLSTM(nn.Module):\n",
    "    def __init__(self, in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout):\n",
    "        super(GNNLSTM, self).__init__()\n",
    "        self.gnn1 = GATv2Conv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=gnn_hidden,\n",
    "            heads=gat_heads,\n",
    "            concat=True,\n",
    "            dropout=gat_dropout,\n",
    "        )\n",
    "        self.gnn2 = GATv2Conv(\n",
    "            in_channels=gnn_hidden * gat_heads,\n",
    "            out_channels=gnn_hidden,\n",
    "            heads=1,\n",
    "            concat=False,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=gnn_hidden,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=lstm_dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(lstm_hidden, 1)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(gnn_hidden * gat_heads)\n",
    "        self.norm2 = nn.LayerNorm(gnn_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gnn_dropout = gnn_dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: (batch_size, seq_len, num_nodes, in_channels)\n",
    "        batch_size, seq_len, num_nodes, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        total_graphs = batch_size * seq_len\n",
    "        x_reshaped = x.reshape(total_graphs, num_nodes, -1)\n",
    "\n",
    "        E = edge_index.size(1)\n",
    "        batched_edge_index = edge_index.unsqueeze(0).repeat(total_graphs, 1, 1)\n",
    "        offsets = (torch.arange(total_graphs, device=device) * num_nodes).view(total_graphs, 1, 1)\n",
    "        batched_edge_index = batched_edge_index + offsets\n",
    "\n",
    "        if E != 0:\n",
    "            batched_edge_index = batched_edge_index.cpu().numpy()\n",
    "            edge_index_final = []\n",
    "            for l in range(batched_edge_index.shape[0]):\n",
    "                for k in range(E):\n",
    "                    edge_index_final.append(np.array([batched_edge_index[l, 0, k], batched_edge_index[l, 1, k]]))\n",
    "            \n",
    "            batched_edge_index = torch.tensor(np.vstack(edge_index_final), device=device).t().contiguous()\n",
    "        \n",
    "        else:\n",
    "            batched_edge_index = batched_edge_index.reshape(2,0)\n",
    "\n",
    "        x_flat = x_reshaped.reshape(total_graphs * num_nodes, -1)\n",
    "\n",
    "        gnn_out = self.gnn1(x_flat, batched_edge_index)\n",
    "        gnn_out = self.norm1(gnn_out)\n",
    "        gnn_out = self.relu(gnn_out)\n",
    "        gnn_out = F.dropout(gnn_out, p=self.gnn_dropout, training=self.training)\n",
    "        gnn_out = self.gnn2(gnn_out, batched_edge_index)\n",
    "        gnn_out = self.norm2(gnn_out)\n",
    "        gnn_out = gnn_out.reshape(total_graphs, num_nodes, -1)\n",
    "        gnn_out = gnn_out.reshape(batch_size, seq_len, num_nodes, -1)\n",
    "\n",
    "        lstm_input = gnn_out.transpose(1, 2).reshape(batch_size * num_nodes, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        pred = self.fc(last_out)\n",
    "        pred = pred.reshape(batch_size, num_nodes)\n",
    "        return pred\n",
    "\n",
    "class LitGNNLSTM(pl.LightningModule):\n",
    "    def __init__(self, in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, learning_rate, edge_index):\n",
    "        super(LitGNNLSTM, self).__init__()\n",
    "        self.model = GNNLSTM(in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                             lstm_hidden, lstm_layers, lstm_dropout)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.register_buffer(\"edge_index\", edge_index)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, self.edge_index)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "def create_model(edge_index, params):\n",
    "    \"\"\"Create the Lightning model using hyperparameters from Optuna.\"\"\"\n",
    "    try:\n",
    "        model = LitGNNLSTM(\n",
    "            in_channels=NODE_FEATURES,\n",
    "            gnn_hidden=params[\"gnn_hidden\"],\n",
    "            gat_heads=params[\"gat_heads\"],\n",
    "            gat_dropout=params[\"gat_dropout\"],\n",
    "            gnn_dropout=params[\"gnn_dropout\"],\n",
    "            lstm_hidden=params[\"lstm_hidden\"],\n",
    "            lstm_layers=params[\"lstm_layers\"],\n",
    "            lstm_dropout=params[\"lstm_dropout\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            edge_index=edge_index,\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in create_model(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer Setup\n",
    "# ---------------------------\n",
    "def create_trainer(max_epochs):\n",
    "    \"\"\"Create a PyTorch Lightning Trainer with early stopping and AMP enabled.\"\"\"\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=EARLY_STOP_DELTA,\n",
    "        patience=EARLY_STOP_PATIENCE,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        precision=\"16-mixed\",\n",
    "        callbacks=[early_stop_callback] if use_validation else None,\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=True,\n",
    "        enable_checkpointing=True,\n",
    "        logger=False,\n",
    "        enable_model_summary=False,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c08f66-6a10-43e2-b74d-4ffed63c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_HORIZON = 2\n",
    "PARAMS_PATH = MAIN_PATH + f\"output/mho/study_results_gat/HORIZON_{PREDICTION_HORIZON}_trials_df.parquet\"\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc25fda-c905-4381-853d-c215b5a55be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_df = pd.read_parquet(PARAMS_PATH).sort_values(\"value\")\n",
    "best_params = params_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9308d3-e91d-4287-81d6-460b65306bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_gat_dropout</th>\n",
       "      <th>params_gat_heads</th>\n",
       "      <th>params_gnn_dropout</th>\n",
       "      <th>params_gnn_hidden</th>\n",
       "      <th>params_graph_threshold</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_lstm_dropout</th>\n",
       "      <th>params_lstm_hidden</th>\n",
       "      <th>params_lstm_layers</th>\n",
       "      <th>user_attrs_epochs</th>\n",
       "      <th>user_attrs_error</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.670953</td>\n",
       "      <td>2025-03-17 18:50:59.548215</td>\n",
       "      <td>2025-03-17 18:51:22.682153</td>\n",
       "      <td>0 days 00:00:23.133938</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>896</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.2</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.688087</td>\n",
       "      <td>2025-03-17 18:54:32.311163</td>\n",
       "      <td>2025-03-17 18:54:55.308663</td>\n",
       "      <td>0 days 00:00:22.997500</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>944</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.5</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.696260</td>\n",
       "      <td>2025-03-17 18:50:06.698199</td>\n",
       "      <td>2025-03-17 18:50:37.057170</td>\n",
       "      <td>0 days 00:00:30.358971</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.1</td>\n",
       "      <td>848</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.2</td>\n",
       "      <td>304</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.706201</td>\n",
       "      <td>2025-03-17 18:56:55.293295</td>\n",
       "      <td>2025-03-17 18:57:26.142928</td>\n",
       "      <td>0 days 00:00:30.849633</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>928</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.6</td>\n",
       "      <td>288</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.798862</td>\n",
       "      <td>2025-03-17 18:54:09.828776</td>\n",
       "      <td>2025-03-17 18:54:32.310618</td>\n",
       "      <td>0 days 00:00:22.481842</td>\n",
       "      <td>48</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>928</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.5</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.467457</td>\n",
       "      <td>2025-03-17 18:42:59.533549</td>\n",
       "      <td>2025-03-17 18:43:11.186058</td>\n",
       "      <td>0 days 00:00:11.652509</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>768</td>\n",
       "      <td>360</td>\n",
       "      <td>0.042529</td>\n",
       "      <td>0.5</td>\n",
       "      <td>656</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>2.625921</td>\n",
       "      <td>2025-03-17 18:59:05.465048</td>\n",
       "      <td>2025-03-17 18:59:15.385531</td>\n",
       "      <td>0 days 00:00:09.920483</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>720</td>\n",
       "      <td>140</td>\n",
       "      <td>0.020678</td>\n",
       "      <td>0.5</td>\n",
       "      <td>608</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>inf</td>\n",
       "      <td>2025-03-17 18:51:34.176185</td>\n",
       "      <td>2025-03-17 18:51:36.355933</td>\n",
       "      <td>0 days 00:00:02.179748</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2</td>\n",
       "      <td>896</td>\n",
       "      <td>280</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 6.07 GiB...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>inf</td>\n",
       "      <td>2025-03-17 18:56:38.666086</td>\n",
       "      <td>2025-03-17 18:56:40.716507</td>\n",
       "      <td>0 days 00:00:02.050421</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2</td>\n",
       "      <td>976</td>\n",
       "      <td>340</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.5</td>\n",
       "      <td>432</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 7.84 GiB...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>inf</td>\n",
       "      <td>2025-03-17 18:59:15.386133</td>\n",
       "      <td>2025-03-17 18:59:16.850331</td>\n",
       "      <td>0 days 00:00:01.464198</td>\n",
       "      <td>48</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>784</td>\n",
       "      <td>660</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.6</td>\n",
       "      <td>448</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 7.46 GiB...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "21      21  0.670953 2025-03-17 18:50:59.548215 2025-03-17 18:51:22.682153   \n",
       "32      32  0.688087 2025-03-17 18:54:32.311163 2025-03-17 18:54:55.308663   \n",
       "19      19  0.696260 2025-03-17 18:50:06.698199 2025-03-17 18:50:37.057170   \n",
       "41      41  0.706201 2025-03-17 18:56:55.293295 2025-03-17 18:57:26.142928   \n",
       "31      31  0.798862 2025-03-17 18:54:09.828776 2025-03-17 18:54:32.310618   \n",
       "..     ...       ...                        ...                        ...   \n",
       "0        0  2.467457 2025-03-17 18:42:59.533549 2025-03-17 18:43:11.186058   \n",
       "49      49  2.625921 2025-03-17 18:59:05.465048 2025-03-17 18:59:15.385531   \n",
       "23      23       inf 2025-03-17 18:51:34.176185 2025-03-17 18:51:36.355933   \n",
       "39      39       inf 2025-03-17 18:56:38.666086 2025-03-17 18:56:40.716507   \n",
       "50      50       inf 2025-03-17 18:59:15.386133 2025-03-17 18:59:16.850331   \n",
       "\n",
       "                 duration  params_batch_size  params_gat_dropout  \\\n",
       "21 0 days 00:00:23.133938                 48                 0.1   \n",
       "32 0 days 00:00:22.997500                 48                 0.1   \n",
       "19 0 days 00:00:30.358971                 48                 0.0   \n",
       "41 0 days 00:00:30.849633                 48                 0.1   \n",
       "31 0 days 00:00:22.481842                 48                 0.3   \n",
       "..                    ...                ...                 ...   \n",
       "0  0 days 00:00:11.652509                 32                 0.1   \n",
       "49 0 days 00:00:09.920483                 16                 0.0   \n",
       "23 0 days 00:00:02.179748                 64                 0.1   \n",
       "39 0 days 00:00:02.050421                 64                 0.5   \n",
       "50 0 days 00:00:01.464198                 48                 0.5   \n",
       "\n",
       "    params_gat_heads  params_gnn_dropout  params_gnn_hidden  \\\n",
       "21                10                 0.2                896   \n",
       "32                12                 0.3                944   \n",
       "19                13                 0.1                848   \n",
       "41                 6                 0.5                928   \n",
       "31                 7                 0.3                928   \n",
       "..               ...                 ...                ...   \n",
       "0                  1                 0.2                768   \n",
       "49                 6                 0.3                720   \n",
       "23                13                 0.2                896   \n",
       "39                12                 0.2                976   \n",
       "50                16                 0.1                784   \n",
       "\n",
       "    params_graph_threshold  params_learning_rate  params_lstm_dropout  \\\n",
       "21                      20              0.000065                  0.2   \n",
       "32                       0              0.000051                  0.5   \n",
       "19                       0              0.000069                  0.2   \n",
       "41                      20              0.000019                  0.6   \n",
       "31                       0              0.000016                  0.5   \n",
       "..                     ...                   ...                  ...   \n",
       "0                      360              0.042529                  0.5   \n",
       "49                     140              0.020678                  0.5   \n",
       "23                     280              0.000085                  0.1   \n",
       "39                     340              0.000038                  0.5   \n",
       "50                     660              0.000049                  0.6   \n",
       "\n",
       "    params_lstm_hidden  params_lstm_layers user_attrs_epochs  \\\n",
       "21                 320                   2                34   \n",
       "32                 320                   2                31   \n",
       "19                 304                   2                43   \n",
       "41                 288                   2                53   \n",
       "31                 320                   2                40   \n",
       "..                 ...                 ...               ...   \n",
       "0                  656                   1                 7   \n",
       "49                 608                   2                 8   \n",
       "23                  16                   2              None   \n",
       "39                 432                   2              None   \n",
       "50                 448                   1              None   \n",
       "\n",
       "                                     user_attrs_error     state  \n",
       "21                                               None  COMPLETE  \n",
       "32                                               None  COMPLETE  \n",
       "19                                               None  COMPLETE  \n",
       "41                                               None  COMPLETE  \n",
       "31                                               None  COMPLETE  \n",
       "..                                                ...       ...  \n",
       "0                                                None  COMPLETE  \n",
       "49                                               None  COMPLETE  \n",
       "23  CUDA out of memory. Tried to allocate 6.07 GiB...  COMPLETE  \n",
       "39  CUDA out of memory. Tried to allocate 7.84 GiB...  COMPLETE  \n",
       "50  CUDA out of memory. Tried to allocate 7.46 GiB...  COMPLETE  \n",
       "\n",
       "[64 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9896d7-d5fa-48c2-b588-a0b6d192b1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_gat_dropout</th>\n",
       "      <th>params_gat_heads</th>\n",
       "      <th>params_gnn_dropout</th>\n",
       "      <th>params_gnn_hidden</th>\n",
       "      <th>params_graph_threshold</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_lstm_dropout</th>\n",
       "      <th>params_lstm_hidden</th>\n",
       "      <th>params_lstm_layers</th>\n",
       "      <th>user_attrs_epochs</th>\n",
       "      <th>user_attrs_error</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.670953</td>\n",
       "      <td>2025-03-17 18:50:59.548215</td>\n",
       "      <td>2025-03-17 18:51:22.682153</td>\n",
       "      <td>0 days 00:00:23.133938</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>896</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.2</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>None</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "21      21  0.670953 2025-03-17 18:50:59.548215 2025-03-17 18:51:22.682153   \n",
       "\n",
       "                 duration  params_batch_size  params_gat_dropout  \\\n",
       "21 0 days 00:00:23.133938                 48                 0.1   \n",
       "\n",
       "    params_gat_heads  params_gnn_dropout  params_gnn_hidden  \\\n",
       "21                10                 0.2                896   \n",
       "\n",
       "    params_graph_threshold  params_learning_rate  params_lstm_dropout  \\\n",
       "21                      20              0.000065                  0.2   \n",
       "\n",
       "    params_lstm_hidden  params_lstm_layers user_attrs_epochs user_attrs_error  \\\n",
       "21                 320                   2                34             None   \n",
       "\n",
       "       state  \n",
       "21  COMPLETE  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48145d1-e618-41d4-8c47-3def9d847231",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": float(best_params[\"params_learning_rate\"].values[0]),\n",
    "    \"gnn_hidden\": int(best_params[\"params_gnn_hidden\"].values[0]),\n",
    "    \"gnn_dropout\": float(best_params[\"params_gnn_dropout\"].values[0]),\n",
    "    \"gat_heads\": int(best_params[\"params_gat_heads\"].values[0]),\n",
    "    \"gat_dropout\": float(best_params[\"params_gat_dropout\"].values[0]),\n",
    "    \"lstm_hidden\": int(best_params[\"params_lstm_hidden\"].values[0]),\n",
    "    \"lstm_dropout\": float(best_params[\"params_lstm_dropout\"].values[0]),\n",
    "    \"lstm_layers\": int(best_params[\"params_lstm_layers\"].values[0]),\n",
    "    \"graph_threshold\": int(best_params[\"params_graph_threshold\"].values[0]),\n",
    "    \"batch_size\": int(best_params[\"params_batch_size\"].values[0]),\n",
    "    \"epochs\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a324a99-c16e-4d4e-aa4b-b1730e2fbb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/notebooks/mho/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad571e9893c4480b3b6fe8619e5da1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "train_loader, _ , test_loader = prepare_data(use_validation=True, prediction_horizon=PREDICTION_HORIZON, batch_size=params[\"batch_size\"])\n",
    "edge_index, _ = prepare_graph(params[\"graph_threshold\"])\n",
    "\n",
    "model = create_model(edge_index, params)\n",
    "trainer = create_trainer(max_epochs=params[\"epochs\"])\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019c82fa-8db2-4fbe-8b55-45c81bdb5a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/notebooks/mho/checkpoints/epoch=49-step=300.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Loaded model weights from the checkpoint at /dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/notebooks/mho/checkpoints/epoch=49-step=300.ckpt\n",
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa56bd74cedf4a89b407724669753606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.67795330286026      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.67795330286026     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.67795330286026}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d052c-8093-401f-b2ac-da001abd2bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5cca96-6660-4178-ae2f-39fc502af281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e9007-7932-4240-a84f-7cc7d9db4c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b6fd4-1345-41cb-a4a9-4c78d615c59b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # Main Routine for Optuna Tuning\n",
    "# # ---------------------------\n",
    "# def main():\n",
    "#         trials_df = study.trials_dataframe()\n",
    "#         output_file = os.path.join(OUTPUT_DIR, f\"HORIZON_{PREDICTION_HORIZON}_trials_df.parquet\")\n",
    "#         trials_df.to_parquet(output_file)\n",
    "#         logging.info(f\"Trials dataframe saved to {output_file}\")\n",
    "\n",
    "#         logging.info(\"Best trial:\")\n",
    "#         best_trial = study.best_trial\n",
    "#         logging.info(f\"  Value: {best_trial.value}\")\n",
    "#         logging.info(\"  Params: \")\n",
    "#         for key, value in best_trial.params.items():\n",
    "#             logging.info(f\"    {key}: {value}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(\"Error in main(): \" + str(e))\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# # ---------------------------\n",
    "# # Run Over Multiple Prediction Horizons\n",
    "# # ---------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     horizons = range(MIN_HORIZON, MAX_HORIZON)\n",
    "#     for h in horizons:\n",
    "#         try:\n",
    "#             PREDICTION_HORIZON = h\n",
    "#             print(f\"\\nTUNING FOR HORIZON {PREDICTION_HORIZON}\\n\")\n",
    "#             logging.info(f\"Optimizing for prediction horizon {PREDICTION_HORIZON}...\")\n",
    "#             main()\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error while optimizing for horizon {h}: {e}\")\n",
    "#             continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
