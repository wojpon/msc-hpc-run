{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c201db8-2bca-4ca8-a654-4f25495a5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import optuna\n",
    "from helper_functions import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MAIN_PATH = \"/dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/\"\n",
    "\n",
    "# ---------------------------\n",
    "# Global settings and seeding\n",
    "# ---------------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Global paths and constants\n",
    "IMPORTS_PATH = os.path.join(MAIN_PATH, \"data/import-volume.csv\")\n",
    "ADJACENCY_MATRIX = pd.read_parquet(os.path.join(MAIN_PATH, \"data/adjacency-matrix.parquet\"))\n",
    "BOOKINGS_PATH = os.path.join(MAIN_PATH, \"data/bookings_data.pkl\")\n",
    "\n",
    "# Global hyperparameters and settings\n",
    "use_validation = False  \n",
    "NODE_FEATURES = 21        # 6 from volume/week encoding + 13 booking features\n",
    "TIME_WINDOW_SIZE = 13\n",
    "LOADERS_WOKRES = 4\n",
    "\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "EARLY_STOP_DELTA = 0.001\n",
    "\n",
    "DIRECT_HORIZONS_TO_PREDICT = 13\n",
    "\n",
    "# ---------------------------\n",
    "# Data and Graph Preparation\n",
    "# ----------------------\n",
    "def get_import_data():\n",
    "    \"\"\"Load and pivot the import volume data.\"\"\"\n",
    "    try:\n",
    "        import_data = pd.read_csv(IMPORTS_PATH)\n",
    "        import_data[\"week\"] = pd.to_datetime(import_data[\"week\"])\n",
    "        import_data[\"pool\"] = import_data[\"pool\"].astype(\"str\")\n",
    "        import_data[\"volume\"] = import_data[\"import\"].astype(\"float\")\n",
    "        import_data.drop(columns=[\"import\"], inplace=True)\n",
    "        import_data = import_data.sort_values(\"week\")\n",
    "        import_data = import_data.loc[\n",
    "            (import_data.week >= pd.to_datetime(\"2017-05-01\")) &\n",
    "            (import_data.week <= pd.to_datetime(\"2024-10-20\"))\n",
    "        ]\n",
    "        logging.info(f\"Imports data range from {import_data['week'].min().strftime('%Y-%m-%d')} \"\n",
    "                     f\"to {import_data['week'].max().strftime('%Y-%m-%d')}\")\n",
    "        import_data = import_data.pivot(index='week', columns='pool', values='volume').T\n",
    "        import_data = import_data.fillna(0)\n",
    "        return import_data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_import_data(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def get_graph_structure(threshold, a):\n",
    "    \"\"\"Construct the graph structure (edge_index and edge_weights) from the adjacency matrix.\"\"\"\n",
    "    try:\n",
    "        a = a.reset_index(drop=True)\n",
    "        a.columns = range(a.shape[1])\n",
    "        a = a.where(pd.notnull(a), a.T)\n",
    "        a = a.to_numpy()\n",
    "        a_filtered = (a > threshold).astype(np.int32)\n",
    "        edge_index = torch.nonzero(torch.tensor(a_filtered, dtype=torch.long), as_tuple=False).t()\n",
    "        edge_weights = []\n",
    "        for e in edge_index.numpy().T:\n",
    "            distance = a[e[0], e[1]]\n",
    "            edge_weights.append(distance)\n",
    "        edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_graph_structure(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def prepare_data(use_validation, prediction_horizon, batch_size):\n",
    "    \"\"\"Preprocess the data, create features, and return DataLoaders.\"\"\"\n",
    "    try:\n",
    "        data = get_import_data()\n",
    "        bookings = pickle.load(open(BOOKINGS_PATH, \"rb\"))\n",
    "        logging.info(f\"Bookings data range from {min(bookings.keys())} to {max(bookings.keys())}\")\n",
    "\n",
    "        if use_validation:\n",
    "            val_weeks = 52\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - val_weeks - test_weeks\n",
    "        else:\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - test_weeks\n",
    "\n",
    "        volume_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "        week_numbers = data.columns.to_series().dt.isocalendar().week.astype(np.float32).values\n",
    "\n",
    "        if use_validation:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            val_volume = volume_tensor[:, train_weeks: train_weeks + val_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks]\n",
    "        else:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        # Normalize volume using training statistics\n",
    "        train_mean = train_volume.mean(dim=1, keepdim=True)\n",
    "        train_std = train_volume.std(dim=1, keepdim=True) + 1e-6\n",
    "        train_volume_norm = (train_volume - train_mean) / train_std\n",
    "        if use_validation:\n",
    "            val_volume_norm = (val_volume - train_mean) / train_std\n",
    "        test_volume_norm = (test_volume - train_mean) / train_std\n",
    "\n",
    "        # Create week encoding features\n",
    "        week_numbers_tensor = torch.tensor(week_numbers, dtype=torch.float32)\n",
    "        sin_week = torch.sin(2 * np.pi * week_numbers_tensor / 52)\n",
    "        cos_week = torch.cos(2 * np.pi * week_numbers_tensor / 52)\n",
    "        holiday_ohe = (week_numbers_tensor == 52).long()\n",
    "        weeks_to_holiday = (52 - week_numbers_tensor) % 52\n",
    "        weeks_from_holiday = (week_numbers_tensor - 52) % 52\n",
    "\n",
    "        def split_features(feature, n_train, n_val, n_test):\n",
    "            train_feat = feature[:n_train]\n",
    "            val_feat = feature[n_train: n_train + n_val]\n",
    "            test_feat = feature[n_train + n_val: n_train + n_val + n_test]\n",
    "            return train_feat, val_feat, test_feat\n",
    "\n",
    "        if use_validation:\n",
    "            train_sin, val_sin, test_sin = split_features(sin_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_cos, val_cos, test_cos = split_features(cos_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_holiday, val_holiday, test_holiday = split_features(holiday_ohe, train_weeks, val_weeks, test_weeks)\n",
    "            train_to, val_to, test_to = split_features(weeks_to_holiday, train_weeks, val_weeks, test_weeks)\n",
    "            train_from, val_from, test_from = split_features(weeks_from_holiday, train_weeks, val_weeks, test_weeks)\n",
    "        else:\n",
    "            train_sin = sin_week[:train_weeks]\n",
    "            test_sin = sin_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_cos = cos_week[:train_weeks]\n",
    "            test_cos = cos_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_holiday = holiday_ohe[:train_weeks]\n",
    "            test_holiday = holiday_ohe[train_weeks: train_weeks + test_weeks]\n",
    "            train_to = weeks_to_holiday[:train_weeks]\n",
    "            test_to = weeks_to_holiday[train_weeks: train_weeks + test_weeks]\n",
    "            train_from = weeks_from_holiday[:train_weeks]\n",
    "            test_from = weeks_from_holiday[train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        def create_week_feature_tensor(week_feat, num_nodes):\n",
    "            if week_feat.dim() == 1:\n",
    "                week_feat = week_feat.unsqueeze(1)\n",
    "            return week_feat.unsqueeze(0).repeat(num_nodes, 1, 1)\n",
    "\n",
    "        num_nodes = 16  # Hard-coded based on the data\n",
    "        train_sin_feat = create_week_feature_tensor(train_sin, num_nodes)\n",
    "        train_cos_feat = create_week_feature_tensor(train_cos, num_nodes)\n",
    "        train_holiday_feat = create_week_feature_tensor(train_holiday, num_nodes)\n",
    "        train_to_feat = create_week_feature_tensor(train_to, num_nodes)\n",
    "        train_from_feat = create_week_feature_tensor(train_from, num_nodes)\n",
    "\n",
    "        if use_validation:\n",
    "            val_sin_feat = create_week_feature_tensor(val_sin, num_nodes)\n",
    "            val_cos_feat = create_week_feature_tensor(val_cos, num_nodes)\n",
    "            val_holiday_feat = create_week_feature_tensor(val_holiday, num_nodes)\n",
    "            val_to_feat = create_week_feature_tensor(val_to, num_nodes)\n",
    "            val_from_feat = create_week_feature_tensor(val_from, num_nodes)\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "        else:\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "\n",
    "        train_volume_feat = train_volume_norm.unsqueeze(2)\n",
    "        if use_validation:\n",
    "            val_volume_feat = val_volume_norm.unsqueeze(2)\n",
    "        test_volume_feat = test_volume_norm.unsqueeze(2)\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            val_data_combined = torch.cat([\n",
    "                val_volume_feat, val_sin_feat, val_cos_feat,\n",
    "                val_holiday_feat, val_to_feat, val_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "\n",
    "        # Process booking features\n",
    "        booking_list = []\n",
    "        for date in data.columns:\n",
    "            booking_df = bookings[date.strftime(\"%Y-%m-%d\")]\n",
    "            booking_list.append(booking_df.values)\n",
    "        booking_array = np.stack(booking_list, axis=1)\n",
    "        bookings_tensor = torch.tensor(booking_array, dtype=torch.float32)\n",
    "\n",
    "        if use_validation:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            val_bookings = bookings_tensor[:, train_weeks: train_weeks + val_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks, :]\n",
    "        else:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks: train_weeks + test_weeks, :]\n",
    "\n",
    "        booking_mean = train_bookings.mean(dim=(0, 1), keepdim=True)\n",
    "        booking_std = train_bookings.std(dim=(0, 1), keepdim=True) + 1e-6\n",
    "        train_bookings = (train_bookings - booking_mean) / booking_std\n",
    "        if use_validation:\n",
    "            val_bookings = (val_bookings - booking_mean) / booking_std\n",
    "        test_bookings = (test_bookings - booking_mean) / booking_std\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            val_data_combined = torch.cat([val_data_combined, val_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "\n",
    "        class TimeSeriesDataset(Dataset):\n",
    "            def __init__(self, data, window_size, horizon):\n",
    "                self.data = data\n",
    "                self.window_size = window_size\n",
    "                self.horizon = horizon  # Now used to predict multiple future steps\n",
    "                self.num_samples = data.shape[1] - window_size - horizon + 1\n",
    "\n",
    "            def __len__(self):\n",
    "                return self.num_samples\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                x = self.data[:, idx: idx + self.window_size, :].transpose(0, 1)\n",
    "                # Now y is a sequence of length `horizon` (i.e., 13 time steps)\n",
    "                y = self.data[:, idx + self.window_size: idx + self.window_size + self.horizon, 0]\n",
    "                return x, y\n",
    "\n",
    "        window_size = TIME_WINDOW_SIZE\n",
    "        train_dataset = TimeSeriesDataset(train_data_combined, window_size, horizon=prediction_horizon)\n",
    "        test_dataset = TimeSeriesDataset(test_data_combined, window_size, horizon=prediction_horizon)\n",
    "        if use_validation:\n",
    "            val_dataset = TimeSeriesDataset(val_data_combined, window_size, horizon=prediction_horizon)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                                  pin_memory=True, num_workers=LOADERS_WOKRES)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "        if use_validation:\n",
    "            val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, \n",
    "                                    pin_memory=True)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        return train_loader, val_loader, test_loader, (train_mean, train_std)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_data(): \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def prepare_graph(threshold):\n",
    "    \"\"\"Prepare the graph structure (edge_index, edge_weights) using the given threshold.\"\"\"\n",
    "    try:\n",
    "        edge_index, edge_weights = get_graph_structure(threshold, ADJACENCY_MATRIX)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_graph(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definitions\n",
    "# ---------------------------\n",
    "class GNNLSTM(nn.Module):\n",
    "    def __init__(self, in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, forecast_horizon):\n",
    "        super(GNNLSTM, self).__init__()\n",
    "        self.gnn1 = GATv2Conv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=gnn_hidden,\n",
    "            heads=gat_heads,\n",
    "            concat=True,\n",
    "            dropout=gat_dropout,\n",
    "        )\n",
    "        self.gnn2 = GATv2Conv(\n",
    "            in_channels=gnn_hidden * gat_heads,\n",
    "            out_channels=gnn_hidden,\n",
    "            heads=1,\n",
    "            concat=False,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=gnn_hidden * gat_heads,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=lstm_dropout,\n",
    "        )\n",
    "        # Now output a vector of length equal to forecast_horizon (i.e., 13 steps)\n",
    "        self.fc = nn.Linear(lstm_hidden, forecast_horizon)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(gnn_hidden * gat_heads)\n",
    "        self.norm2 = nn.LayerNorm(gnn_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gnn_dropout = gnn_dropout\n",
    "\n",
    "    def forward(self, x, edge_index, return_attention_weights=False):\n",
    "        # x: (batch_size, seq_len, num_nodes, in_channels)\n",
    "        batch_size, seq_len, num_nodes, _ = x.shape\n",
    "\n",
    "        gnn_outputs = []\n",
    "        for t in range(seq_len):\n",
    "            gnn_out_batch = []\n",
    "            for b in range(batch_size):\n",
    "                node_features = x[b, t]\n",
    "                if return_attention_weights:\n",
    "                    gnn_out, attention_tuple = self.gnn1(node_features, edge_index, return_attention_weights=return_attention_weights)\n",
    "                else:\n",
    "                    gnn_out = self.gnn1(node_features, edge_index)\n",
    "                gnn_out = self.norm1(gnn_out)\n",
    "                # gnn_out = self.relu(gnn_out)\n",
    "                # gnn_out = F.dropout(gnn_out, p=self.gnn_dropout, training=self.training)\n",
    "                # gnn_out = self.gnn2(gnn_out, edge_index)\n",
    "                # gnn_out = self.norm2(gnn_out)\n",
    "                gnn_out_batch.append(gnn_out.unsqueeze(0))\n",
    "            gnn_out_batch = torch.cat(gnn_out_batch, dim=0)\n",
    "            gnn_outputs.append(gnn_out_batch.unsqueeze(1))\n",
    "        gnn_out = torch.cat(gnn_outputs, dim=1)\n",
    "\n",
    "        lstm_input = gnn_out.transpose(1, 2).reshape(batch_size * num_nodes, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        pred = self.fc(lstm_out)\n",
    "        # Reshape to (batch_size, num_nodes, forecast_horizon)\n",
    "        pred = pred.reshape(batch_size, num_nodes, -1)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            return pred, attention_tuple\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "class LitGNNLSTM(pl.LightningModule):\n",
    "    def __init__(self, in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, learning_rate, edge_index, forecast_horizon):\n",
    "        super(LitGNNLSTM, self).__init__()\n",
    "        self.model = GNNLSTM(in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                             lstm_hidden, lstm_layers, lstm_dropout, forecast_horizon)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.register_buffer(\"edge_index\", edge_index)\n",
    "\n",
    "    def forward(self, x, return_attention_weights=False):\n",
    "        return self.model(x, self.edge_index, return_attention_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y now has shape (batch_size, num_nodes, forecast_horizon)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1, verbose=True)\n",
    "        return optimizer\n",
    "         \n",
    "\n",
    "def create_model(edge_index, params):\n",
    "    \"\"\"Create the Lightning model using hyperparameters from Optuna.\"\"\"\n",
    "    try:\n",
    "        model = LitGNNLSTM(\n",
    "            in_channels=NODE_FEATURES,\n",
    "            gnn_hidden=params[\"gnn_hidden\"],\n",
    "            gat_heads=params[\"gat_heads\"],\n",
    "            gat_dropout=params[\"gat_dropout\"],\n",
    "            gnn_dropout=params[\"gnn_dropout\"],\n",
    "            lstm_hidden=params[\"lstm_hidden\"],\n",
    "            lstm_layers=params[\"lstm_layers\"],\n",
    "            lstm_dropout=params[\"lstm_dropout\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            edge_index=edge_index,\n",
    "            forecast_horizon=DIRECT_HORIZONS_TO_PREDICT,  # Now predicting 13 steps at once\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in create_model(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer Setup\n",
    "# ---------------------------\n",
    "def create_trainer(max_epochs):\n",
    "    \"\"\"Create a PyTorch Lightning Trainer with early stopping and AMP enabled.\"\"\"\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=EARLY_STOP_DELTA,\n",
    "        patience=EARLY_STOP_PATIENCE,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        precision=\"16-mixed\",\n",
    "        callbacks=[early_stop_callback],\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=False,\n",
    "        enable_checkpointing=True,\n",
    "        logger=True,\n",
    "        enable_model_summary=False,\n",
    "        default_root_dir=\"lightning_gat_weights/\"\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa72aa-80e8-4401-872c-5b19142dd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS_PATH = MAIN_PATH + f\"output_old/sho/study_results_gat/sho_gat_trials_df.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdec1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_df = pd.read_parquet(PARAMS_PATH).sort_values(\"value\")\n",
    "# best_params = params_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02834299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"learning_rate\": float(best_params[\"params_learning_rate\"].values[0]),\n",
    "#     \"gnn_hidden\": int(best_params[\"params_gnn_hidden\"].values[0]),\n",
    "#     \"gnn_dropout\": float(best_params[\"params_gnn_dropout\"].values[0]),\n",
    "#     \"gat_heads\": int(best_params[\"params_gat_heads\"].values[0]),\n",
    "#     \"gat_dropout\": float(best_params[\"params_gat_dropout\"].values[0]),\n",
    "#     \"lstm_hidden\": int(best_params[\"params_lstm_hidden\"].values[0]),\n",
    "#     \"lstm_dropout\": float(best_params[\"params_lstm_dropout\"].values[0]),\n",
    "#     \"lstm_layers\": int(best_params[\"params_lstm_layers\"].values[0]),\n",
    "#     \"graph_threshold\": int(best_params[\"params_graph_threshold\"].values[0]),\n",
    "#     \"batch_size\": int(best_params[\"params_batch_size\"].values[0]),\n",
    "#     \"epochs\": 300,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387454f-7e37-42e5-bf8f-8dac2884232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 1.0521479961657395e-05,\n",
    "    \"gnn_hidden\": 704,\n",
    "    \"gnn_dropout\": 0.0,\n",
    "    \"gat_heads\": 4,\n",
    "    \"gat_dropout\": 0.2,\n",
    "    \"lstm_hidden\": 640,\n",
    "    \"lstm_dropout\": 0.1,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"graph_threshold\": 0.1,\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc23ca4-961e-480c-a547-1a91589b560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader , test_loader, train_statistics = prepare_data(use_validation=use_validation, prediction_horizon=DIRECT_HORIZONS_TO_PREDICT, batch_size=params[\"batch_size\"])\n",
    "train_mean, train_std = train_statistics\n",
    "edge_index, _ = prepare_graph(params[\"graph_threshold\"])\n",
    "\n",
    "model = create_model(edge_index, params)\n",
    "trainer = create_trainer(max_epochs=params[\"epochs\"])\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "attention_tuples = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        preds, attention_tuple = model(x, return_attention_weights=True)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.numpy())\n",
    "        attention_tuples.append(attention_tuple)\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "all_preds = (all_preds.T * train_std.numpy() + train_mean.numpy()).T\n",
    "all_targets = (all_targets.T * train_std.numpy() + train_mean.numpy()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124485d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_coefs = []\n",
    "for b in range(len(attention_tuples)):\n",
    "    att_tuple = attention_tuples[b]\n",
    "    att_w_idx = att_tuple[0].numpy()[:, :-16]\n",
    "    att_coef = att_tuple[1].numpy()[:-16].mean(axis=1).reshape(-1, 1)\n",
    "    att_coefs.append(att_coef)\n",
    "att_coefs = np.hstack(att_coefs).mean(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ae2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_import_data()\n",
    "\n",
    "edges = att_w_idx\n",
    "weights = att_coef.flatten()\n",
    "\n",
    "matrix = np.zeros((16, 16))\n",
    "\n",
    "for i in range(edges.shape[1]):\n",
    "    source = int(edges[0, i])\n",
    "    target = int(edges[1, i])\n",
    "    matrix[source, target] = weights[i]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(matrix, cmap=\"viridis\", vmin=0, vmax=1,\n",
    "                 xticklabels=data.index.str[2:].tolist(), yticklabels=data.index.str[2:].tolist(),\n",
    "                 annot=True, fmt=\".2f\")\n",
    "\n",
    "# Move the x-axis labels to the top.\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "ax.set_xlabel(\"Target Node\")\n",
    "ax.set_ylabel(\"Source Node\")\n",
    "ax.set_title(\"Attention Weights\", pad=30)  # Increase pad to give space for the title\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d38fbd-be87-4b97-90d9-489a57fe90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_w_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5821e7e-430d-4515-ba42-25372a6c076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191197c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = create_predictions(all_preds, all_targets)\n",
    "results_horizon = create_results_horizon(predictions)\n",
    "results_pool = create_results_pool(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2429c5-7f99-4787-a2c4-48cc27d7359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7901cb-bb97-4f64-86c7-e57e894a11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_errors(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee91995",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(predictions, \"GBDPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948885ea-9694-4ac0-a6fb-80789e58184f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be7677-1eb0-4309-a911-59006fa39918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
