{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615abf57-ed5e-446d-bee9-9b1eaa7e4cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import optuna\n",
    "from helper_functions import *\n",
    "\n",
    "\n",
    "MAIN_PATH = \"/dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/\"\n",
    "\n",
    "# ---------------------------\n",
    "# Global settings and seeding\n",
    "# ---------------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Global paths and constants\n",
    "IMPORTS_PATH = os.path.join(MAIN_PATH, \"data/import-volume.csv\")\n",
    "ADJACENCY_MATRIX = pd.read_parquet(os.path.join(MAIN_PATH, \"data/adjacency-matrix.parquet\"))\n",
    "BOOKINGS_PATH = os.path.join(MAIN_PATH, \"data/bookings_data.pkl\")\n",
    "\n",
    "# Global hyperparameters and settings\n",
    "use_validation = False  \n",
    "NODE_FEATURES = 21        # 6 from volume/week encoding + 13 booking features\n",
    "TIME_WINDOW_SIZE = 13\n",
    "LOADERS_WOKRES = 4\n",
    "\n",
    "MIN_HORIZON = 1\n",
    "MAX_HORIZON = 14\n",
    "\n",
    "EARLY_STOP_PATIENCE = 7\n",
    "EARLY_STOP_DELTA = 0.001\n",
    "\n",
    "DIRECT_HORIZONS_TO_PREDICT = 13\n",
    "\n",
    "# ---------------------------\n",
    "# Data and Graph Preparation\n",
    "# ---------------------------\n",
    "def get_import_data():\n",
    "    \"\"\"Load and pivot the import volume data.\"\"\"\n",
    "    try:\n",
    "        import_data = pd.read_csv(IMPORTS_PATH)\n",
    "        import_data[\"week\"] = pd.to_datetime(import_data[\"week\"])\n",
    "        import_data[\"pool\"] = import_data[\"pool\"].astype(\"str\")\n",
    "        import_data[\"volume\"] = import_data[\"import\"].astype(\"float\")\n",
    "        import_data.drop(columns=[\"import\"], inplace=True)\n",
    "        import_data = import_data.sort_values(\"week\")\n",
    "        import_data = import_data.loc[\n",
    "            (import_data.week >= pd.to_datetime(\"2017-05-01\")) &\n",
    "            (import_data.week <= pd.to_datetime(\"2024-10-20\"))\n",
    "        ]\n",
    "        logging.info(f\"Imports data range from {import_data['week'].min().strftime('%Y-%m-%d')} \"\n",
    "                     f\"to {import_data['week'].max().strftime('%Y-%m-%d')}\")\n",
    "        import_data = import_data.pivot(index='week', columns='pool', values='volume').T\n",
    "        import_data = import_data.fillna(0)\n",
    "        return import_data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_import_data(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def get_graph_structure(threshold, a):\n",
    "    \"\"\"Construct the graph structure (edge_index and edge_weights) from the adjacency matrix.\"\"\"\n",
    "    try:\n",
    "        a = a.reset_index(drop=True)\n",
    "        a.columns = range(a.shape[1])\n",
    "        a = a.where(pd.notnull(a), a.T)\n",
    "        a = a.to_numpy()\n",
    "        a_filtered = (a > threshold).astype(np.int32)\n",
    "        edge_index = torch.nonzero(torch.tensor(a_filtered, dtype=torch.long), as_tuple=False).t()\n",
    "        edge_weights = []\n",
    "        for e in edge_index.numpy().T:\n",
    "            distance = a[e[0], e[1]]\n",
    "            edge_weights.append(distance)\n",
    "        edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "        # epsilon = 1e-8\n",
    "        # edge_weights = 10 / (edge_weights + epsilon)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_graph_structure(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def prepare_data(use_validation, prediction_horizon, batch_size):\n",
    "    \"\"\"Preprocess the data, create features, and return DataLoaders.\"\"\"\n",
    "    try:\n",
    "        data = get_import_data()\n",
    "        bookings = pickle.load(open(BOOKINGS_PATH, \"rb\"))\n",
    "        logging.info(f\"Bookings data range from {min(bookings.keys())} to {max(bookings.keys())}\")\n",
    "\n",
    "        if use_validation:\n",
    "            val_weeks = 52\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - val_weeks - test_weeks\n",
    "        else:\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - test_weeks\n",
    "\n",
    "        volume_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "        week_numbers = data.columns.to_series().dt.isocalendar().week.astype(np.float32).values\n",
    "\n",
    "        if use_validation:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            val_volume = volume_tensor[:, train_weeks: train_weeks + val_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks]\n",
    "        else:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        # Normalize volume using training statistics\n",
    "        train_mean = train_volume.mean(dim=1, keepdim=True)\n",
    "        train_std = train_volume.std(dim=1, keepdim=True) + 1e-6\n",
    "        train_volume_norm = (train_volume - train_mean) / train_std\n",
    "        if use_validation:\n",
    "            val_volume_norm = (val_volume - train_mean) / train_std\n",
    "        test_volume_norm = (test_volume - train_mean) / train_std\n",
    "\n",
    "        # Create week encoding features\n",
    "        week_numbers_tensor = torch.tensor(week_numbers, dtype=torch.float32)\n",
    "        sin_week = torch.sin(2 * np.pi * week_numbers_tensor / 52)\n",
    "        cos_week = torch.cos(2 * np.pi * week_numbers_tensor / 52)\n",
    "        holiday_ohe = (week_numbers_tensor == 52).long()\n",
    "        weeks_to_holiday = (52 - week_numbers_tensor) % 52\n",
    "        weeks_from_holiday = (week_numbers_tensor - 52) % 52\n",
    "\n",
    "        def split_features(feature, n_train, n_val, n_test):\n",
    "            train_feat = feature[:n_train]\n",
    "            val_feat = feature[n_train: n_train + n_val]\n",
    "            test_feat = feature[n_train + n_val: n_train + n_val + n_test]\n",
    "            return train_feat, val_feat, test_feat\n",
    "\n",
    "        if use_validation:\n",
    "            train_sin, val_sin, test_sin = split_features(sin_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_cos, val_cos, test_cos = split_features(cos_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_holiday, val_holiday, test_holiday = split_features(holiday_ohe, train_weeks, val_weeks, test_weeks)\n",
    "            train_to, val_to, test_to = split_features(weeks_to_holiday, train_weeks, val_weeks, test_weeks)\n",
    "            train_from, val_from, test_from = split_features(weeks_from_holiday, train_weeks, val_weeks, test_weeks)\n",
    "        else:\n",
    "            train_sin = sin_week[:train_weeks]\n",
    "            test_sin = sin_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_cos = cos_week[:train_weeks]\n",
    "            test_cos = cos_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_holiday = holiday_ohe[:train_weeks]\n",
    "            test_holiday = holiday_ohe[train_weeks: train_weeks + test_weeks]\n",
    "            train_to = weeks_to_holiday[:train_weeks]\n",
    "            test_to = weeks_to_holiday[train_weeks: train_weeks + test_weeks]\n",
    "            train_from = weeks_from_holiday[:train_weeks]\n",
    "            test_from = weeks_from_holiday[train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        def create_week_feature_tensor(week_feat, num_nodes):\n",
    "            if week_feat.dim() == 1:\n",
    "                week_feat = week_feat.unsqueeze(1)\n",
    "            return week_feat.unsqueeze(0).repeat(num_nodes, 1, 1)\n",
    "\n",
    "        num_nodes = 16  # Hard-coded based on the data\n",
    "        train_sin_feat = create_week_feature_tensor(train_sin, num_nodes)\n",
    "        train_cos_feat = create_week_feature_tensor(train_cos, num_nodes)\n",
    "        train_holiday_feat = create_week_feature_tensor(train_holiday, num_nodes)\n",
    "        train_to_feat = create_week_feature_tensor(train_to, num_nodes)\n",
    "        train_from_feat = create_week_feature_tensor(train_from, num_nodes)\n",
    "\n",
    "        if use_validation:\n",
    "            val_sin_feat = create_week_feature_tensor(val_sin, num_nodes)\n",
    "            val_cos_feat = create_week_feature_tensor(val_cos, num_nodes)\n",
    "            val_holiday_feat = create_week_feature_tensor(val_holiday, num_nodes)\n",
    "            val_to_feat = create_week_feature_tensor(val_to, num_nodes)\n",
    "            val_from_feat = create_week_feature_tensor(val_from, num_nodes)\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "        else:\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "\n",
    "        train_volume_feat = train_volume_norm.unsqueeze(2)\n",
    "        if use_validation:\n",
    "            val_volume_feat = val_volume_norm.unsqueeze(2)\n",
    "        test_volume_feat = test_volume_norm.unsqueeze(2)\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            val_data_combined = torch.cat([\n",
    "                val_volume_feat, val_sin_feat, val_cos_feat,\n",
    "                val_holiday_feat, val_to_feat, val_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "\n",
    "        # Process booking features\n",
    "        booking_list = []\n",
    "        for date in data.columns:\n",
    "            booking_df = bookings[date.strftime(\"%Y-%m-%d\")]\n",
    "            booking_list.append(booking_df.values)\n",
    "        booking_array = np.stack(booking_list, axis=1)\n",
    "        bookings_tensor = torch.tensor(booking_array, dtype=torch.float32)\n",
    "\n",
    "        if use_validation:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            val_bookings = bookings_tensor[:, train_weeks: train_weeks + val_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks, :]\n",
    "        else:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks: train_weeks + test_weeks, :]\n",
    "\n",
    "        booking_mean = train_bookings.mean(dim=(0, 1), keepdim=True)\n",
    "        booking_std = train_bookings.std(dim=(0, 1), keepdim=True) + 1e-6\n",
    "        train_bookings = (train_bookings - booking_mean) / booking_std\n",
    "        if use_validation:\n",
    "            val_bookings = (val_bookings - booking_mean) / booking_std\n",
    "        test_bookings = (test_bookings - booking_mean) / booking_std\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            val_data_combined = torch.cat([val_data_combined, val_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "\n",
    "        class TimeSeriesDataset(Dataset):\n",
    "            def __init__(self, data, window_size, horizon):\n",
    "                self.data = data\n",
    "                self.window_size = window_size\n",
    "                self.horizon = horizon  # Now used to predict multiple future steps\n",
    "                self.num_samples = data.shape[1] - window_size - horizon + 1\n",
    "\n",
    "            def __len__(self):\n",
    "                return self.num_samples\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                x = self.data[:, idx: idx + self.window_size, :].transpose(0, 1)\n",
    "                # Now y is a sequence of length `horizon` (i.e., 13 time steps)\n",
    "                y = self.data[:, idx + self.window_size: idx + self.window_size + self.horizon, 0]\n",
    "                return x, y\n",
    "\n",
    "        window_size = TIME_WINDOW_SIZE\n",
    "        train_dataset = TimeSeriesDataset(train_data_combined, window_size, horizon=prediction_horizon)\n",
    "        test_dataset = TimeSeriesDataset(test_data_combined, window_size, horizon=prediction_horizon)\n",
    "        if use_validation:\n",
    "            val_dataset = TimeSeriesDataset(val_data_combined, window_size, horizon=prediction_horizon)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                  pin_memory=True, num_workers=LOADERS_WOKRES)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=LOADERS_WOKRES)\n",
    "        if use_validation:\n",
    "            val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, \n",
    "                                    pin_memory=True)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        return train_loader, val_loader, test_loader, (train_mean, train_std)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_data(): \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def prepare_graph(threshold):\n",
    "    \"\"\"Prepare the graph structure (edge_index, edge_weights) using the given threshold.\"\"\"\n",
    "    try:\n",
    "        edge_index, edge_weights = get_graph_structure(threshold, ADJACENCY_MATRIX)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_graph(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definitions\n",
    "# ---------------------------\n",
    "class GNNLSTM(nn.Module):\n",
    "    def __init__(self, in_channels, gnn_hidden, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, forecast_horizon):\n",
    "        super(GNNLSTM, self).__init__()\n",
    "        self.gnn1 = GCNConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=gnn_hidden,\n",
    "        )\n",
    "        self.gnn2 = GCNConv(\n",
    "            in_channels=gnn_hidden,\n",
    "            out_channels=gnn_hidden,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=gnn_hidden,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=lstm_dropout,\n",
    "        )\n",
    "        # Now output a vector of length equal to forecast_horizon (i.e., 13 steps)\n",
    "        self.fc = nn.Linear(lstm_hidden, forecast_horizon)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(gnn_hidden)\n",
    "        self.norm2 = nn.LayerNorm(gnn_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gnn_dropout = gnn_dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x: (batch_size, seq_len, num_nodes, in_channels)\n",
    "        batch_size, seq_len, num_nodes, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        total_graphs = batch_size * seq_len\n",
    "        x_reshaped = x.reshape(total_graphs, num_nodes, -1)\n",
    "\n",
    "        E = edge_index.size(1)\n",
    "        batched_edge_index = edge_index.unsqueeze(0).repeat(total_graphs, 1, 1)\n",
    "        offsets = (torch.arange(total_graphs, device=device) * num_nodes).view(total_graphs, 1, 1)\n",
    "        batched_edge_index = batched_edge_index + offsets\n",
    "        \n",
    "        \n",
    "        if E != 0:\n",
    "            batched_edge_index = batched_edge_index.cpu().numpy()\n",
    "            edge_index_final = []\n",
    "            for l in range(batched_edge_index.shape[0]):\n",
    "                for k in range(E):\n",
    "                    edge_index_final.append(np.array([batched_edge_index[l, 0, k], batched_edge_index[l, 1, k]]))\n",
    "            \n",
    "            batched_edge_index = torch.tensor(np.vstack(edge_index_final), device=device).t().contiguous()\n",
    "        \n",
    "        else:\n",
    "            batched_edge_index = batched_edge_index.reshape(2,0)\n",
    "            \n",
    "            \n",
    "        # Batch edge weights if provided\n",
    "        if edge_weight is not None:\n",
    "            batched_edge_weight = edge_weight.unsqueeze(0).repeat(total_graphs, 1)\n",
    "            batched_edge_weight = batched_edge_weight.reshape(total_graphs * E)\n",
    "        else:\n",
    "            batched_edge_weight = None\n",
    "            \n",
    "        x_flat = x_reshaped.reshape(total_graphs * num_nodes, -1)\n",
    "\n",
    "        gnn_out = self.gnn1(x_flat, batched_edge_index, edge_weight=batched_edge_weight)\n",
    "        gnn_out = self.norm1(gnn_out)\n",
    "        # gnn_out = self.relu(gnn_out)\n",
    "        # gnn_out = F.dropout(gnn_out, p=self.gnn_dropout, training=self.training)\n",
    "        # gnn_out = self.gnn2(gnn_out, batched_edge_index, edge_weight=batched_edge_weight)\n",
    "        # gnn_out = self.norm2(gnn_out)\n",
    "        gnn_out = gnn_out.reshape(total_graphs, num_nodes, -1)\n",
    "        gnn_out = gnn_out.reshape(batch_size, seq_len, num_nodes, -1)\n",
    "\n",
    "        lstm_input = gnn_out.transpose(1, 2).reshape(batch_size * num_nodes, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        pred = self.fc(last_out)\n",
    "        # Reshape to (batch_size, num_nodes, forecast_horizon)\n",
    "        pred = pred.reshape(batch_size, num_nodes, -1)\n",
    "        return pred\n",
    "\n",
    "class LitGNNLSTM(pl.LightningModule):\n",
    "    def __init__(self, in_channels, gnn_hidden, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, learning_rate, edge_index, edge_weight, forecast_horizon):\n",
    "        super(LitGNNLSTM, self).__init__()\n",
    "        self.model = GNNLSTM(in_channels, gnn_hidden, gnn_dropout,\n",
    "                             lstm_hidden, lstm_layers, lstm_dropout, forecast_horizon)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.register_buffer(\"edge_index\", edge_index)\n",
    "        self.register_buffer(\"edge_weight\", edge_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, self.edge_index, self.edge_weight)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y now has shape (batch_size, num_nodes, forecast_horizon)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "def create_model(edge_index, edge_weight, params):\n",
    "    \"\"\"Create the Lightning model using hyperparameters from Optuna.\"\"\"\n",
    "    try:\n",
    "        model = LitGNNLSTM(\n",
    "            in_channels=NODE_FEATURES,\n",
    "            gnn_hidden=params[\"gnn_hidden\"],\n",
    "            gnn_dropout=params[\"gnn_dropout\"],\n",
    "            lstm_hidden=params[\"lstm_hidden\"],\n",
    "            lstm_layers=params[\"lstm_layers\"],\n",
    "            lstm_dropout=params[\"lstm_dropout\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            edge_index=edge_index,\n",
    "            edge_weight=edge_weight,\n",
    "            forecast_horizon=DIRECT_HORIZONS_TO_PREDICT,  # Now predicting 13 steps at once\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in create_model(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer Setup\n",
    "# ---------------------------\n",
    "def create_trainer(max_epochs):\n",
    "    \"\"\"Create a PyTorch Lightning Trainer with early stopping and AMP enabled.\"\"\"\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=EARLY_STOP_DELTA,\n",
    "        patience=EARLY_STOP_PATIENCE,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        precision=\"16-mixed\",\n",
    "        callbacks=[early_stop_callback],\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=False,\n",
    "        enable_checkpointing=True,\n",
    "        logger=True,\n",
    "        enable_model_summary=False,\n",
    "        default_root_dir=\"lightning_gcn/\"\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c08f66-6a10-43e2-b74d-4ffed63c32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS_PATH = MAIN_PATH + f\"output/sho/study_results_gcn/sho_gcn_trials_df.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d80f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_df = pd.read_parquet(PARAMS_PATH).sort_values(\"value\")\n",
    "# best_params = params_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93160628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0aeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"learning_rate\": float(best_params[\"params_learning_rate\"].values[0]),\n",
    "#     \"gnn_hidden\": int(best_params[\"params_gnn_hidden\"].values[0]),\n",
    "#     \"gnn_dropout\": float(best_params[\"params_gnn_dropout\"].values[0]),\n",
    "#     \"lstm_hidden\": int(best_params[\"params_lstm_hidden\"].values[0]),\n",
    "#     \"lstm_dropout\": float(best_params[\"params_lstm_dropout\"].values[0]),\n",
    "#     \"lstm_layers\": int(best_params[\"params_lstm_layers\"].values[0]),\n",
    "#     \"graph_threshold\": int(best_params[\"params_graph_threshold\"].values[0]),\n",
    "#     \"batch_size\": int(best_params[\"params_batch_size\"].values[0]),\n",
    "#     \"epochs\": int(np.min([20, best_params[\"user_attrs_epochs\"].values[0]])),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ac1cfd-92ac-4a15-a4a0-bf0538819431",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"gnn_hidden\": 512,\n",
    "    \"gnn_dropout\": 0.4,\n",
    "    \"lstm_hidden\": 512,\n",
    "    \"lstm_dropout\": 0.6,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"graph_threshold\": 100,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a324a99-c16e-4d4e-aa4b-b1730e2fbb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-03-20 14:44:04.184833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742478244.205871  359501 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742478244.212326  359501 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742478244.228796  359501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742478244.228816  359501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742478244.228819  359501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742478244.228821  359501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-20 14:44:04.234320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Metric val_loss improved. New best score: 1.591\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.590\n",
      "Metric val_loss improved by 0.132 >= min_delta = 0.001. New best score: 1.458\n",
      "Metric val_loss improved by 0.601 >= min_delta = 0.001. New best score: 0.857\n",
      "Metric val_loss improved by 0.107 >= min_delta = 0.001. New best score: 0.750\n",
      "Metric val_loss improved by 0.014 >= min_delta = 0.001. New best score: 0.736\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.001. New best score: 0.728\n",
      "Metric val_loss improved by 0.054 >= min_delta = 0.001. New best score: 0.674\n",
      "Metric val_loss improved by 0.025 >= min_delta = 0.001. New best score: 0.648\n",
      "Metric val_loss improved by 0.008 >= min_delta = 0.001. New best score: 0.640\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, train_statistics = prepare_data(use_validation=use_validation, prediction_horizon=DIRECT_HORIZONS_TO_PREDICT, batch_size=params[\"batch_size\"])\n",
    "train_mean, train_std = train_statistics\n",
    "edge_index, edge_weight = prepare_graph(params[\"graph_threshold\"])\n",
    "\n",
    "model = create_model(edge_index, edge_weight, params)\n",
    "trainer = create_trainer(max_epochs=params[\"epochs\"])\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d052c-8093-401f-b2ac-da001abd2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        preds = model(x)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "all_preds = (all_preds.T * train_std.numpy() + train_mean.numpy()).T\n",
    "all_targets = (all_targets.T * train_std.numpy() + train_mean.numpy()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e400b-ce65-4df9-af28-996107a91de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5cca96-6660-4178-ae2f-39fc502af281",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = create_predictions(all_preds, all_targets)\n",
    "results_horizon = create_results_horizon(predictions)\n",
    "results_pool = create_results_pool(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e9007-7932-4240-a84f-7cc7d9db4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49505bcb-7e6a-463b-acaa-b651ee8b8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b58697",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_errors(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(predictions, \"GBFXS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec83be0-0198-4c32-b1cb-eb42795f47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE GCN 0 km\n",
    "# Test RMSE: 225.8216\n",
    "# Test MPE: -13.5227\n",
    "# Test WMAPE: 20.5187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d61f57-8ad6-48fa-a1be-dba83cc6577c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f17c7-0fa4-49ca-8d56-a0c6117df79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE GCN 800 km\n",
    "# Test RMSE: 206.1053\n",
    "# Test MPE: -15.1339\n",
    "# Test WMAPE: 20.9579"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
