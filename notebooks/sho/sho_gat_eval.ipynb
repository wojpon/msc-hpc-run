{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c201db8-2bca-4ca8-a654-4f25495a5ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import optuna\n",
    "\n",
    "MAIN_PATH = \"/dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/\"\n",
    "\n",
    "# ---------------------------\n",
    "# Global settings and seeding\n",
    "# ---------------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Global paths and constants\n",
    "IMPORTS_PATH = os.path.join(MAIN_PATH, \"data/import-volume.csv\")\n",
    "ADJACENCY_MATRIX = pd.read_parquet(os.path.join(MAIN_PATH, \"data/adjacency-matrix.parquet\"))\n",
    "BOOKINGS_PATH = os.path.join(MAIN_PATH, \"data/bookings_data.pkl\")\n",
    "\n",
    "# Global hyperparameters and settings\n",
    "use_validation = False  \n",
    "NODE_FEATURES = 19        # 6 from volume/week encoding + 13 booking features\n",
    "TIME_WINDOW_SIZE = 13\n",
    "LOADERS_WOKRES = 4\n",
    "\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "EARLY_STOP_DELTA = 0.001\n",
    "\n",
    "# ---------------------------\n",
    "# Data and Graph Preparation\n",
    "# ---------------------------\n",
    "def get_import_data():\n",
    "    \"\"\"Load and pivot the import volume data.\"\"\"\n",
    "    try:\n",
    "        import_data = pd.read_csv(IMPORTS_PATH)\n",
    "        import_data[\"week\"] = pd.to_datetime(import_data[\"week\"])\n",
    "        import_data[\"pool\"] = import_data[\"pool\"].astype(\"str\")\n",
    "        import_data[\"volume\"] = import_data[\"import\"].astype(\"float\")\n",
    "        import_data.drop(columns=[\"import\"], inplace=True)\n",
    "        import_data = import_data.sort_values(\"week\")\n",
    "        import_data = import_data.loc[\n",
    "            (import_data.week >= pd.to_datetime(\"2017-05-01\")) &\n",
    "            (import_data.week <= pd.to_datetime(\"2024-10-20\"))\n",
    "        ]\n",
    "        logging.info(f\"Imports data range from {import_data['week'].min().strftime('%Y-%m-%d')} \"\n",
    "                     f\"to {import_data['week'].max().strftime('%Y-%m-%d')}\")\n",
    "        import_data = import_data.pivot(index='week', columns='pool', values='volume').T\n",
    "        import_data = import_data.fillna(0)\n",
    "        return import_data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_import_data(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def get_graph_structure(threshold, a):\n",
    "    \"\"\"Construct the graph structure (edge_index and edge_weights) from the adjacency matrix.\"\"\"\n",
    "    try:\n",
    "        a = a.reset_index(drop=True)\n",
    "        a.columns = range(a.shape[1])\n",
    "        a = a.where(pd.notnull(a), a.T)\n",
    "        a = a.to_numpy()\n",
    "        a_filtered = (a < threshold).astype(np.int32)\n",
    "        edge_index = torch.nonzero(torch.tensor(a_filtered, dtype=torch.long), as_tuple=False).t()\n",
    "        edge_weights = []\n",
    "        for e in edge_index.numpy().T:\n",
    "            distance = a[e[0], e[1]]\n",
    "            edge_weights.append(distance)\n",
    "        edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in get_graph_structure(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "def prepare_data(use_validation, prediction_horizon, batch_size):\n",
    "    \"\"\"Preprocess the data, create features, and return DataLoaders.\"\"\"\n",
    "    try:\n",
    "        data = get_import_data()\n",
    "        bookings = pickle.load(open(BOOKINGS_PATH, \"rb\"))\n",
    "        logging.info(f\"Bookings data range from {min(bookings.keys())} to {max(bookings.keys())}\")\n",
    "\n",
    "        if use_validation:\n",
    "            val_weeks = 52\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - val_weeks - test_weeks\n",
    "        else:\n",
    "            test_weeks = 77\n",
    "            train_weeks = data.shape[1] - test_weeks\n",
    "\n",
    "        volume_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "        week_numbers = data.columns.to_series().dt.isocalendar().week.astype(np.float32).values\n",
    "\n",
    "        if use_validation:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            val_volume = volume_tensor[:, train_weeks: train_weeks + val_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks]\n",
    "        else:\n",
    "            train_volume = volume_tensor[:, :train_weeks]\n",
    "            test_volume = volume_tensor[:, train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        # Normalize volume using training statistics\n",
    "        train_mean = train_volume.mean(dim=1, keepdim=True)\n",
    "        train_std = train_volume.std(dim=1, keepdim=True) + 1e-6\n",
    "        train_volume_norm = (train_volume - train_mean) / train_std\n",
    "        if use_validation:\n",
    "            val_volume_norm = (val_volume - train_mean) / train_std\n",
    "        test_volume_norm = (test_volume - train_mean) / train_std\n",
    "\n",
    "        # Create week encoding features\n",
    "        week_numbers_tensor = torch.tensor(week_numbers, dtype=torch.float32)\n",
    "        sin_week = torch.sin(2 * np.pi * week_numbers_tensor / 52)\n",
    "        cos_week = torch.cos(2 * np.pi * week_numbers_tensor / 52)\n",
    "        holiday_ohe = (week_numbers_tensor == 52).long()\n",
    "        weeks_to_holiday = (52 - week_numbers_tensor) % 52\n",
    "        weeks_from_holiday = (week_numbers_tensor - 52) % 52\n",
    "\n",
    "        def split_features(feature, n_train, n_val, n_test):\n",
    "            train_feat = feature[:n_train]\n",
    "            val_feat = feature[n_train: n_train + n_val]\n",
    "            test_feat = feature[n_train + n_val: n_train + n_val + n_test]\n",
    "            return train_feat, val_feat, test_feat\n",
    "\n",
    "        if use_validation:\n",
    "            train_sin, val_sin, test_sin = split_features(sin_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_cos, val_cos, test_cos = split_features(cos_week, train_weeks, val_weeks, test_weeks)\n",
    "            train_holiday, val_holiday, test_holiday = split_features(holiday_ohe, train_weeks, val_weeks, test_weeks)\n",
    "            train_to, val_to, test_to = split_features(weeks_to_holiday, train_weeks, val_weeks, test_weeks)\n",
    "            train_from, val_from, test_from = split_features(weeks_from_holiday, train_weeks, val_weeks, test_weeks)\n",
    "        else:\n",
    "            train_sin = sin_week[:train_weeks]\n",
    "            test_sin = sin_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_cos = cos_week[:train_weeks]\n",
    "            test_cos = cos_week[train_weeks: train_weeks + test_weeks]\n",
    "            train_holiday = holiday_ohe[:train_weeks]\n",
    "            test_holiday = holiday_ohe[train_weeks: train_weeks + test_weeks]\n",
    "            train_to = weeks_to_holiday[:train_weeks]\n",
    "            test_to = weeks_to_holiday[train_weeks: train_weeks + test_weeks]\n",
    "            train_from = weeks_from_holiday[:train_weeks]\n",
    "            test_from = weeks_from_holiday[train_weeks: train_weeks + test_weeks]\n",
    "\n",
    "        def create_week_feature_tensor(week_feat, num_nodes):\n",
    "            if week_feat.dim() == 1:\n",
    "                week_feat = week_feat.unsqueeze(1)\n",
    "            return week_feat.unsqueeze(0).repeat(num_nodes, 1, 1)\n",
    "\n",
    "        num_nodes = 16  # Hard-coded based on the data\n",
    "        train_sin_feat = create_week_feature_tensor(train_sin, num_nodes)\n",
    "        train_cos_feat = create_week_feature_tensor(train_cos, num_nodes)\n",
    "        train_holiday_feat = create_week_feature_tensor(train_holiday, num_nodes)\n",
    "        train_to_feat = create_week_feature_tensor(train_to, num_nodes)\n",
    "        train_from_feat = create_week_feature_tensor(train_from, num_nodes)\n",
    "\n",
    "        if use_validation:\n",
    "            val_sin_feat = create_week_feature_tensor(val_sin, num_nodes)\n",
    "            val_cos_feat = create_week_feature_tensor(val_cos, num_nodes)\n",
    "            val_holiday_feat = create_week_feature_tensor(val_holiday, num_nodes)\n",
    "            val_to_feat = create_week_feature_tensor(val_to, num_nodes)\n",
    "            val_from_feat = create_week_feature_tensor(val_from, num_nodes)\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "        else:\n",
    "            test_sin_feat = create_week_feature_tensor(test_sin, num_nodes)\n",
    "            test_cos_feat = create_week_feature_tensor(test_cos, num_nodes)\n",
    "            test_holiday_feat = create_week_feature_tensor(test_holiday, num_nodes)\n",
    "            test_to_feat = create_week_feature_tensor(test_to, num_nodes)\n",
    "            test_from_feat = create_week_feature_tensor(test_from, num_nodes)\n",
    "\n",
    "        train_volume_feat = train_volume_norm.unsqueeze(2)\n",
    "        if use_validation:\n",
    "            val_volume_feat = val_volume_norm.unsqueeze(2)\n",
    "        test_volume_feat = test_volume_norm.unsqueeze(2)\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            val_data_combined = torch.cat([\n",
    "                val_volume_feat, val_sin_feat, val_cos_feat,\n",
    "                val_holiday_feat, val_to_feat, val_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([\n",
    "                train_volume_feat, train_sin_feat, train_cos_feat,\n",
    "                train_holiday_feat, train_to_feat, train_from_feat\n",
    "            ], dim=2)\n",
    "            test_data_combined = torch.cat([\n",
    "                test_volume_feat, test_sin_feat, test_cos_feat,\n",
    "                test_holiday_feat, test_to_feat, test_from_feat\n",
    "            ], dim=2)\n",
    "\n",
    "        # Process booking features\n",
    "        booking_list = []\n",
    "        for date in data.columns:\n",
    "            booking_df = bookings[date.strftime(\"%Y-%m-%d\")]\n",
    "            booking_list.append(booking_df.values)\n",
    "        booking_array = np.stack(booking_list, axis=1)\n",
    "        bookings_tensor = torch.tensor(booking_array, dtype=torch.float32)\n",
    "\n",
    "        if use_validation:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            val_bookings = bookings_tensor[:, train_weeks: train_weeks + val_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks + val_weeks: train_weeks + val_weeks + test_weeks, :]\n",
    "        else:\n",
    "            train_bookings = bookings_tensor[:, :train_weeks, :]\n",
    "            test_bookings = bookings_tensor[:, train_weeks: train_weeks + test_weeks, :]\n",
    "\n",
    "        booking_mean = train_bookings.mean(dim=(0, 1), keepdim=True)\n",
    "        booking_std = train_bookings.std(dim=(0, 1), keepdim=True) + 1e-6\n",
    "        train_bookings = (train_bookings - booking_mean) / booking_std\n",
    "        if use_validation:\n",
    "            val_bookings = (val_bookings - booking_mean) / booking_std\n",
    "        test_bookings = (test_bookings - booking_mean) / booking_std\n",
    "\n",
    "        if use_validation:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            val_data_combined = torch.cat([val_data_combined, val_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "        else:\n",
    "            train_data_combined = torch.cat([train_data_combined, train_bookings], dim=2)\n",
    "            test_data_combined = torch.cat([test_data_combined, test_bookings], dim=2)\n",
    "\n",
    "        class TimeSeriesDataset(Dataset):\n",
    "            def __init__(self, data, window_size, horizon):\n",
    "                self.data = data\n",
    "                self.window_size = window_size\n",
    "                self.horizon = horizon  # Now used to predict multiple future steps\n",
    "                self.num_samples = data.shape[1] - window_size - horizon + 1\n",
    "\n",
    "            def __len__(self):\n",
    "                return self.num_samples\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                x = self.data[:, idx: idx + self.window_size, :].transpose(0, 1)\n",
    "                # Now y is a sequence of length `horizon` (i.e., 13 time steps)\n",
    "                y = self.data[:, idx + self.window_size: idx + self.window_size + self.horizon, 0]\n",
    "                return x, y\n",
    "\n",
    "        window_size = TIME_WINDOW_SIZE\n",
    "        train_dataset = TimeSeriesDataset(train_data_combined, window_size, horizon=prediction_horizon)\n",
    "        test_dataset = TimeSeriesDataset(test_data_combined, window_size, horizon=prediction_horizon)\n",
    "        if use_validation:\n",
    "            val_dataset = TimeSeriesDataset(val_data_combined, window_size, horizon=prediction_horizon)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                  pin_memory=True, num_workers=LOADERS_WOKRES)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "        if use_validation:\n",
    "            val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, \n",
    "                                    pin_memory=True)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_data(): \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def prepare_graph(threshold):\n",
    "    \"\"\"Prepare the graph structure (edge_index, edge_weights) using the given threshold.\"\"\"\n",
    "    try:\n",
    "        edge_index, edge_weights = get_graph_structure(threshold, ADJACENCY_MATRIX)\n",
    "        return edge_index, edge_weights\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in prepare_graph(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "# ---------------------------\n",
    "# Model Definitions\n",
    "# ---------------------------\n",
    "class GNNLSTM(nn.Module):\n",
    "    def __init__(self, in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, forecast_horizon):\n",
    "        super(GNNLSTM, self).__init__()\n",
    "        self.gnn1 = GATv2Conv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=gnn_hidden,\n",
    "            heads=gat_heads,\n",
    "            concat=True,\n",
    "            dropout=gat_dropout,\n",
    "        )\n",
    "        self.gnn2 = GATv2Conv(\n",
    "            in_channels=gnn_hidden * gat_heads,\n",
    "            out_channels=gnn_hidden,\n",
    "            heads=1,\n",
    "            concat=False,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=gnn_hidden,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=lstm_dropout,\n",
    "        )\n",
    "        # Now output a vector of length equal to forecast_horizon (i.e., 13 steps)\n",
    "        self.fc = nn.Linear(lstm_hidden, forecast_horizon)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(gnn_hidden * gat_heads)\n",
    "        self.norm2 = nn.LayerNorm(gnn_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gnn_dropout = gnn_dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: (batch_size, seq_len, num_nodes, in_channels)\n",
    "        batch_size, seq_len, num_nodes, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        total_graphs = batch_size * seq_len\n",
    "        x_reshaped = x.reshape(total_graphs, num_nodes, -1)\n",
    "\n",
    "        E = edge_index.size(1)\n",
    "        batched_edge_index = edge_index.unsqueeze(0).repeat(total_graphs, 1, 1)\n",
    "        offsets = (torch.arange(total_graphs, device=device) * num_nodes).view(total_graphs, 1, 1)\n",
    "        batched_edge_index = batched_edge_index + offsets\n",
    "        \n",
    "        \n",
    "        if E != 0:\n",
    "            batched_edge_index = batched_edge_index.cpu().numpy()\n",
    "            edge_index_final = []\n",
    "            for l in range(batched_edge_index.shape[0]):\n",
    "                for k in range(E):\n",
    "                    edge_index_final.append(np.array([batched_edge_index[l, 0, k], batched_edge_index[l, 1, k]]))\n",
    "            \n",
    "            batched_edge_index = torch.tensor(np.vstack(edge_index_final), device=device).t().contiguous()\n",
    "        \n",
    "        else:\n",
    "            batched_edge_index = batched_edge_index.reshape(2,0)\n",
    "            \n",
    "        x_flat = x_reshaped.reshape(total_graphs * num_nodes, -1)\n",
    "\n",
    "        gnn_out = self.gnn1(x_flat, batched_edge_index)\n",
    "        gnn_out = self.norm1(gnn_out)\n",
    "        gnn_out = self.relu(gnn_out)\n",
    "        gnn_out = F.dropout(gnn_out, p=self.gnn_dropout, training=self.training)\n",
    "        gnn_out = self.gnn2(gnn_out, batched_edge_index)\n",
    "        gnn_out = self.norm2(gnn_out)\n",
    "        gnn_out = gnn_out.reshape(total_graphs, num_nodes, -1)\n",
    "        gnn_out = gnn_out.reshape(batch_size, seq_len, num_nodes, -1)\n",
    "\n",
    "        lstm_input = gnn_out.transpose(1, 2).reshape(batch_size * num_nodes, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        pred = self.fc(last_out)\n",
    "        # Reshape to (batch_size, num_nodes, forecast_horizon)\n",
    "        pred = pred.reshape(batch_size, num_nodes, -1)\n",
    "        return pred\n",
    "\n",
    "class LitGNNLSTM(pl.LightningModule):\n",
    "    def __init__(self, in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                 lstm_hidden, lstm_layers, lstm_dropout, learning_rate, edge_index, forecast_horizon):\n",
    "        super(LitGNNLSTM, self).__init__()\n",
    "        self.model = GNNLSTM(in_channels, gnn_hidden, gat_heads, gat_dropout, gnn_dropout,\n",
    "                             lstm_hidden, lstm_layers, lstm_dropout, forecast_horizon)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.register_buffer(\"edge_index\", edge_index)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, self.edge_index)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y now has shape (batch_size, num_nodes, forecast_horizon)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "def create_model(edge_index, params):\n",
    "    \"\"\"Create the Lightning model using hyperparameters from Optuna.\"\"\"\n",
    "    try:\n",
    "        model = LitGNNLSTM(\n",
    "            in_channels=NODE_FEATURES,\n",
    "            gnn_hidden=params[\"gnn_hidden\"],\n",
    "            gat_heads=params[\"gat_heads\"],\n",
    "            gat_dropout=params[\"gat_dropout\"],\n",
    "            gnn_dropout=params[\"gnn_dropout\"],\n",
    "            lstm_hidden=params[\"lstm_hidden\"],\n",
    "            lstm_layers=params[\"lstm_layers\"],\n",
    "            lstm_dropout=params[\"lstm_dropout\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            edge_index=edge_index,\n",
    "            forecast_horizon=DIRECT_HORIZONS_TO_PREDICT,  # Now predicting 13 steps at once\n",
    "        )\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in create_model(): \" + str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer Setup\n",
    "# ---------------------------\n",
    "def create_trainer(max_epochs):\n",
    "    \"\"\"Create a PyTorch Lightning Trainer with early stopping and AMP enabled.\"\"\"\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=EARLY_STOP_DELTA,\n",
    "        patience=EARLY_STOP_PATIENCE,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        precision=\"16-mixed\",\n",
    "        callbacks=[early_stop_callback] if use_validation else None,\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=False,\n",
    "        enable_checkpointing=True,\n",
    "        logger=False,\n",
    "        enable_model_summary=False,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83dfb388-fc04-48a5-94be-8be03cf49384",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"gnn_hidden\": 512,\n",
    "    \"gnn_dropout\": 0.5,\n",
    "    \"gat_heads\": 8,\n",
    "    \"gat_dropout\": 0.6,\n",
    "    \"lstm_hidden\": 512,\n",
    "    \"lstm_dropout\": 0.6,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"graph_threshold\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94fa72aa-80e8-4401-872c-5b19142dd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECT_HORIZONS_TO_PREDICT = 13\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cc23ca4-961e-480c-a547-1a91589b560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-17 20:48:11,062 [INFO] Imports data range from 2017-05-07 to 2024-10-20\n",
      "2025-03-17 20:48:11,132 [INFO] Bookings data range from 2017-05-07 to 2024-10-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/notebooks/sho/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "train_loader, _ , test_loader = prepare_data(use_validation=True, prediction_horizon=DIRECT_HORIZONS_TO_PREDICT, batch_size=params[\"batch_size\"])\n",
    "edge_index, _ = prepare_graph(params[\"graph_threshold\"])\n",
    "\n",
    "model = create_model(edge_index, params)\n",
    "trainer = create_trainer(max_epochs=EPOCHS)\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daccee8f-fbb3-4795-9991-7bb3094c7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/notebooks/sho/checkpoints/epoch=19-step=160.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Loaded model weights from the checkpoint at /dtu/3d-imaging-center/courses/02509/groups/group10/msc-hpc-run/notebooks/sho/checkpoints/epoch=19-step=160.ckpt\n",
      "/zhome/a9/1/194879/.local/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0903782844543457     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0903782844543457    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.0903782844543457}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c2f5-77ad-4046-81f4-8f7ae63e4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # Main Routine for Optuna Tuning\n",
    "# # ---------------------------\n",
    "# def main():\n",
    "#     try:\n",
    "#         trials_df = study.trials_dataframe()\n",
    "#         output_file = os.path.join(OUTPUT_DIR, f\"sho_gat_trials_df.parquet\")\n",
    "#         trials_df.to_parquet(output_file)\n",
    "#         logging.info(f\"Trials dataframe saved to {output_file}\")\n",
    "\n",
    "#         logging.info(\"Best trial:\")\n",
    "#         best_trial = study.best_trial\n",
    "#         logging.info(f\"  Value: {best_trial.value}\")\n",
    "#         logging.info(\"  Params: \")\n",
    "#         for key, value in best_trial.params.items():\n",
    "#             logging.info(f\"    {key}: {value}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(\"Error in main(): \" + str(e))\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# # ---------------------------\n",
    "# # Run Over Single Prediction Horizon (13 weeks)\n",
    "# # ---------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         main()\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
